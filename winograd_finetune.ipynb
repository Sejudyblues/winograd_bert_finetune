{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "#  p1: should we use softmax to get the loss?\n",
    "#  ans: yes\n",
    "\n",
    "#  p2: Then, log P(c|s) is computed as the average of log-probabilities of each composing token.\n",
    "# There is a lot of \"the\" case\n",
    "\n",
    "# max_candidate length\n",
    "\n",
    "# Joan and Susan case?\n",
    "\n",
    "#  p3: do not know how to slice the tensor in a easy way\n",
    "# ans: masked tensor matmul\n",
    "#############################################\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "from pytorch_transformers import (WEIGHTS_NAME, BertConfig,\n",
    "                                  BertForMaskedLM, BertTokenizer)\n",
    "\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "import argparse\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tensorboardX import SummaryWriter\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop up example\n",
    "\n",
    "# config = BertConfig.from_pretrained('bert-base-uncased')\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertForMaskedLM(config)\n",
    "# input_ids = torch.tensor(tokenizer.encode(\"Hello, my dog is cute. Your dog is not\")).unsqueeze(0)  # Batch size 1\n",
    "\n",
    "# # # masked = torch.tensor([-1, -1, -1, -1, 0, -1])\n",
    "\n",
    "# outputs = model(input_ids, masked_lm_labels=input_ids)\n",
    "\n",
    "# loss, prediction_scores = outputs[:2]\n",
    "\n",
    "# # print(input_ids)\n",
    "# # print(type(loss))\n",
    "# print(prediction_scores)\n",
    "# prob = F.softmax(prediction_scores[0], dim=0)\n",
    "# print(prob)\n",
    "\n",
    "# prob2 = F.softmax(prediction_scores, dim=1)\n",
    "# print(prob2)\n",
    "\n",
    "# print(type(prediction_scores.tolist()[0][4][2]))\n",
    "\n",
    "# tokenizer.encode(\"Joan\")\n",
    "# print(tokenizer.convert_ids_to_tokens([7437]))\n",
    "# seq = tokenizer.tokenize(\"dog is here\")\n",
    "# seq2 = tokenizer.convert_tokens_to_ids(\"dog is here\")\n",
    "# print(type(seq))\n",
    "\n",
    "# mask = [-1] * len(seq)\n",
    "# # mask[seq.index(2003)] = 1\n",
    "\n",
    "# print(mask)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(0.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(p1s, p2s, alpha, beta):\n",
    "    # possible improvement if we use matrix computation\n",
    "    m_p1 = np.mean(np.log(p1s)).item()\n",
    "    m_p2 = np.mean(np.log(p2s)).item()\n",
    "    loss = -1 * m_p1 + alpha * max(0, m_p2 - m_p1 + beta)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 0, 0, 0, 0, 0, 0], dtype=int64), array([1, 2, 3, 4, 5, 6, 7], dtype=int64))\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# # t = torch.Tensor([1, 2, 3])\n",
    "# # print ((t == 2).nonzero())\n",
    "\n",
    "t = np.array([[6294, -1, -1,-1,  -1,   -1,   -1,   -1]])\n",
    "print(np.where(t==-1))\n",
    "t = np.delete(t, np.where(t == -1),axis=None)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "class Args:\n",
    "    \n",
    "    data_dir = \".\\examples\\glue_data\\WINO\"\n",
    "    model_name = 'bert-base-uncased'\n",
    "    output_dir = \".\\examples\\glue_data\\WNLI_out\"\n",
    "    task_name = \"wino\"\n",
    "    output_mode = \"classification\"\n",
    "    \n",
    "    max_seq_length = 80\n",
    "    max_cand_length = 8\n",
    "    do_train = True\n",
    "    do_eval = True\n",
    "    evaluate_during_training = False\n",
    "    do_lower_case = True\n",
    "    \n",
    "    per_gpu_train_batch_size = 4\n",
    "    per_gpu_eval_batch_size = 4\n",
    "    gradient_accumulation_steps = 1\n",
    "    \n",
    "    # for loss\n",
    "    alpha = 5\n",
    "    beta= 0.1\n",
    "    \n",
    "    learning_rate = 2e-5\n",
    "    weight_decay = 0\n",
    "    adam_epsilon = 1e-8\n",
    "    max_grad_norm = 1.0\n",
    "    num_train_epochs = 3\n",
    "    max_steps = 1\n",
    "    warmup_steps = 0\n",
    "    \n",
    "    logging_steps = 50\n",
    "    save_steps = 50\n",
    "    \n",
    "    n_gpu = 1\n",
    "    \n",
    "    eval_all_checkpoints = False\n",
    "    no_cuda = False\n",
    "    overwrite_output_dir = True\n",
    "    overwrite_cache = False\n",
    "\n",
    "    fp16 = False\n",
    "    fp16_opt_level = False\n",
    "    local_rank = -1\n",
    "    server_ip = \"\"\n",
    "    server_port = \"\"\n",
    "\n",
    "args=Args()\n",
    "\n",
    "seed = 42\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if args.n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader Utilization\n",
    "import csv\n",
    "import sys\n",
    "from io import open\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, label, first_token, second_token, pronoun, text_b=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: 0 means the first token is right,1 otherwise\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "        self.first_candidate = first_token\n",
    "        self.second_candidate = second_token\n",
    "        self.pronoun = pronoun\n",
    "     \n",
    "    \n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, masked_lm_labels, label_id, first_token_id, second_token_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.masked_lm_labels = masked_lm_labels\n",
    "        \n",
    "        self.label_id = label_id\n",
    "        self.first_token_id = first_token_id\n",
    "        self.second_token_id = second_token_id\n",
    "        \n",
    "        \n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "        \n",
    "\n",
    "class WinoProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the Wino.\"\"\"\n",
    "    # data type: tsv: guid - text - label - first cand - second cand\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, line[0])\n",
    "            text = line[1]\n",
    "            # text_b = line[2]\n",
    "            label = line[2]\n",
    "            first_token = line[3]\n",
    "            second_token = line[4]\n",
    "            pronoun = line[5]\n",
    "            \n",
    "            examples.append(\n",
    "                InputExample(guid,text, label, first_token, second_token, pronoun))\n",
    "        return examples\n",
    "\n",
    "    \n",
    "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
    "                                 tokenizer, output_mode,\n",
    "                                 cls_token_at_end=False, pad_on_left=False,\n",
    "                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
    "                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
    "                                 cls_token_segment_id=1, pad_token_segment_id=0,\n",
    "                                 mask_padding_with_zero=True):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 10000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "        pronoun = example.pronoun\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = tokens_a + [sep_token]\n",
    "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "        \n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [sep_token]\n",
    "            segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
    "\n",
    "        if cls_token_at_end:\n",
    "            tokens = tokens + [cls_token]\n",
    "            segment_ids = segment_ids + [cls_token_segment_id]\n",
    "        else:\n",
    "            tokens = [cls_token] + tokens\n",
    "            segment_ids = [cls_token_segment_id] + segment_ids\n",
    "        \n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        \n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "        else:\n",
    "            input_ids = input_ids + ([pad_token] * padding_length)\n",
    "            segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        \n",
    "\n",
    "        mask = [-1] * len(input_ids)\n",
    "        mask[input_ids.index(tokenizer.encode(pronoun)[0])] = 0\n",
    "        first_token_id = tokenizer.encode(example.first_candidate)\n",
    "        second_token_id = tokenizer.encode(example.second_candidate)\n",
    "        \n",
    "        while len(first_token_id) < 8:\n",
    "            first_token_id.append(-1)\n",
    "        \n",
    "        while len(second_token_id) < 8:\n",
    "            second_token_id.append(-1)\n",
    "\n",
    "        if output_mode == \"classification\":\n",
    "            label_id = label_map[example.label]\n",
    "        elif output_mode == \"regression\":\n",
    "            label_id = float(example.label)\n",
    "        else:\n",
    "            raise KeyError(output_mode)\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              masked_lm_labels=mask,\n",
    "                              label_id=label_id,\n",
    "                              first_token_id=first_token_id,\n",
    "                              second_token_id=second_token_id))\n",
    "        \n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()    \n",
    "\n",
    "            \n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def acc_and_f1(preds, labels):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"acc_and_f1\": (acc + f1) / 2,\n",
    "    }\n",
    "\n",
    "\n",
    "def pearson_and_spearman(preds, labels):\n",
    "    pearson_corr = pearsonr(preds, labels)[0]\n",
    "    spearman_corr = spearmanr(preds, labels)[0]\n",
    "    return {\n",
    "        \"pearson\": pearson_corr,\n",
    "        \"spearmanr\": spearman_corr,\n",
    "        \"corr\": (pearson_corr + spearman_corr) / 2,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics(task_name, preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    \n",
    "    if task_name == \"wino\":\n",
    "        return {\"acc\": simple_accuracy(preds, labels)}\n",
    "\n",
    "\n",
    "processors = {\n",
    "    \"wino\": WinoProcessor\n",
    "}\n",
    "\n",
    "output_modes = {\n",
    "    \"wino\": \"classification\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2019 04:39:23 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "07/27/2019 04:39:24 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at C:\\Users\\xzhao\\.cache\\torch\\pytorch_transformers\\4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
      "07/27/2019 04:39:24 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/27/2019 04:39:25 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at C:\\Users\\xzhao\\.cache\\torch\\pytorch_transformers\\26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "# setting up envir\n",
    "\n",
    "if os.path.exists(args.output_dir) and os.listdir(args.output_dir) and args.do_train and not args.overwrite_output_dir:\n",
    "    raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args.output_dir))\n",
    "\n",
    "# Setup distant debugging if needed\n",
    "if args.server_ip and args.server_port:\n",
    "    # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "    import ptvsd\n",
    "    print(\"Waiting for debugger attach\")\n",
    "    ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "    ptvsd.wait_for_attach()\n",
    "\n",
    "# Setup CUDA, GPU & distributed training\n",
    "if args.local_rank == -1 or args.no_cuda:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "    args.n_gpu = torch.cuda.device_count()\n",
    "else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    device = torch.device(\"cuda\", args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "    args.n_gpu = 1\n",
    "args.device = device\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "                args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "if args.local_rank not in [-1, 0]:\n",
    "    torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "    \n",
    "    \n",
    "num_labels = 2\n",
    "config = BertConfig.from_pretrained(args.model_name)\n",
    "tokenizer = BertTokenizer.from_pretrained(args.model_name)\n",
    "model = BertForMaskedLM(config)\n",
    "\n",
    "if args.local_rank == 0:\n",
    "    torch.distributed.barrier()\n",
    "    \n",
    "model.to(args.device)\n",
    "if args.local_rank != -1:\n",
    "    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.local_rank],\n",
    "                                                      output_device=args.local_rank,\n",
    "                                                      find_unused_parameters=True)\n",
    "elif args.n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, train_dataset, model, tokenizer):\n",
    "    \n",
    "    \"\"\" Train the model \"\"\"\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer = SummaryWriter()\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "\n",
    "    if args.max_steps > 0:\n",
    "        t_total = args.max_steps\n",
    "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "        \n",
    "    # Prepare optimizer and schedule (linear warmup and decay)\n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args.weight_decay},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
    "    \n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "        \n",
    "    # Train!\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                   args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0])\n",
    "    # set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "#             inputs = {'input_ids':      batch[0],\n",
    "#                       'attention_mask': batch[1],\n",
    "#                       'token_type_ids': batch[2],  \n",
    "#                       'labels':         batch[3]}\n",
    "            \n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'masked_lm_labels': batch[1]}\n",
    "            \n",
    "            input_ids = batch[0]\n",
    "            mask_lm_labels = batch[1]\n",
    "            \n",
    "            correct_token_labels = batch[2]\n",
    "            \n",
    "            # Assume that there is only one token for pronoun\n",
    "           \n",
    "            batch_first_token_ids = batch[3].cpu().detach().numpy()\n",
    "            batch_second_token_ids = batch[4].cpu().detach().numpy()\n",
    "\n",
    "            \n",
    "            ouputs = model(**inputs)\n",
    "            scores = F.softmax(ouputs[1], dim=1)\n",
    "            scores = scores.cpu().detach().numpy()\n",
    "\n",
    "            \n",
    "            loss = torch.tensor(0.0)\n",
    "            \n",
    "            for i in range(len(batch[0])):\n",
    "                batch_real_first = np.delete(batch_first_token_ids[i], np.where(batch_first_token_ids[i] == -1))\n",
    "                batch_real_second = np.delete(batch_second_token_ids[i], np.where(batch_second_token_ids[i] == -1))\n",
    "                pronoun_index = (mask_lm_labels[i] == 0).nonzero()\n",
    "                \n",
    "                if correct_token_labels[i] == 0:\n",
    "                    true_probs = scores[i][pronoun_index][batch_real_first]\n",
    "                    wrong_probs = scores[i][pronoun_index][batch_real_first]\n",
    "                elif correct_token_labels[i] == 1:\n",
    "                    wrong_probs = scores[i][pronoun_index][batch_real_second]\n",
    "                    true_probs = scores[i][pronoun_index][batch_real_second]\n",
    "                \n",
    "                loss += get_loss(true_probs, wrong_probs, args.alpha, args.beta)\n",
    "            \n",
    "            loss = loss / len(batch[0])\n",
    "            loss = torch.autograd.Variable(loss, requires_grad = True)\n",
    "            \n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
    "                    # Log metrics\n",
    "                    if args.local_rank == -1 and args.evaluate_during_training:  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(args, model, tokenizer)\n",
    "                        for key, value in results.items():\n",
    "                            tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n",
    "                    tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar('loss', (tr_loss - logging_loss)/args.logging_steps, global_step)\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args.output_dir, 'checkpoint-{}'.format(global_step))\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "            if args.max_steps > 0 and global_step > args.max_steps:\n",
    "                epoch_iterator.close()\n",
    "                break\n",
    "        if args.max_steps > 0 and global_step > args.max_steps:\n",
    "            train_iterator.close()\n",
    "            break\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, model, tokenizer, prefix=\"\"):\n",
    "    \n",
    "    eval_task_names = [args.task_name]\n",
    "    eval_outputs_dirs = [args.output_dir]\n",
    "                         \n",
    "    results = {}\n",
    "    for eval_task, eval_output_dir in zip(eval_task_names, eval_outputs_dirs):\n",
    "        eval_dataset = load_and_cache_examples(args, eval_task, tokenizer, evaluate=True)\n",
    "\n",
    "        if not os.path.exists(eval_output_dir) and args.local_rank in [-1, 0]:\n",
    "            os.makedirs(eval_output_dir)\n",
    "\n",
    "        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "        # Note that DistributedSampler samples randomly\n",
    "        eval_sampler = SequentialSampler(eval_dataset) if args.local_rank == -1 else DistributedSampler(eval_dataset)\n",
    "        eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "        # Eval!\n",
    "        logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "        logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "        eval_loss = 0.0\n",
    "        nb_eval_steps = 0\n",
    "        preds = None\n",
    "        out_label_ids = None\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            model.eval()\n",
    "            batch = tuple(t.to(args.device) for t in batch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                inputs = {'input_ids':      batch[0],\n",
    "                      'masked_lm_labels': batch[1]}\n",
    "\n",
    "                \n",
    "                input_ids = batch[0]\n",
    "                mask_lm_labels = batch[1]\n",
    "                \n",
    "                tmp_eval_loss = torch.tensor(0.0)\n",
    "            \n",
    "                correct_token_labels = batch[2]\n",
    "            \n",
    "                # Assume that there is only one token for pronoun\n",
    "                batch_first_token_ids = batch[3].cpu().detach().numpy()\n",
    "                batch_second_token_ids = batch[4].cpu().detach().numpy()\n",
    "\n",
    "                ouputs = model(**inputs)\n",
    "                scores = F.softmax(ouputs[1], dim=1)\n",
    "                scores = scores.cpu().detach().numpy()\n",
    "\n",
    "                for i in range(len(batch[0])):\n",
    "                    \n",
    "                    batch_real_first = np.delete(batch_first_token_ids[i], np.where(batch_first_token_ids[i] == -1))\n",
    "                    batch_real_second = np.delete(batch_second_token_ids[i], np.where(batch_second_token_ids[i] == -1))\n",
    "                    \n",
    "                    pronoun_index = (mask_lm_labels[i] == 0).nonzero()\n",
    "                    \n",
    "                    p1 = scores[i][pronoun_index][batch_real_first]\n",
    "                    p2 = scores[i][pronoun_index][batch_real_second]\n",
    "                    \n",
    "                    if correct_token_labels[i] == 0:\n",
    "                        true_probs = p1\n",
    "                        wrong_probs = p2\n",
    "                    elif correct_token_labels[i] == 1:\n",
    "                        wrong_probs = p1\n",
    "                        true_probs = p2\n",
    "\n",
    "                    tmp_eval_loss += get_loss(true_probs, wrong_probs, args.alpha, args.beta)\n",
    "                    \n",
    "                    if preds is None:\n",
    "                        mp1 = np.mean(np.log(p1)).item()\n",
    "                        mp2 = np.mean(np.log(p2)).item()\n",
    "                        \n",
    "                        preds = bool(mp1 > mp2)\n",
    "                        \n",
    "                    else:\n",
    "                        mp1 = np.mean(np.log(p1)).item()\n",
    "                        mp2 = np.mean(np.log(p2)).item()\n",
    "                        preds = np.append(preds, bool(mp1 > mp2))\n",
    "                        \n",
    "                if out_label_ids is None:\n",
    "                    out_label_ids = batch[2].detach().cpu().numpy()\n",
    "                else:\n",
    "                    out_label_ids = np.append(out_label_ids, batch[2].detach().cpu().numpy(), axis=0)\n",
    "                    \n",
    "                tmp_eval_loss = tmp_eval_loss / len(batch[0])                \n",
    "                \n",
    "                eval_loss += tmp_eval_loss.mean().item()\n",
    "            \n",
    "            \n",
    "            nb_eval_steps += 1\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        \n",
    "#         if args.output_mode == \"classification\":\n",
    "#             preds = np.argmax(preds, axis=1)\n",
    "#         elif args.output_mode == \"regression\":\n",
    "#             preds = np.squeeze(preds)\n",
    "        result = compute_metrics(eval_task, preds, out_label_ids)\n",
    "        results.update(result)\n",
    "\n",
    "        output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(args, task, tokenizer, evaluate=False):\n",
    "    \n",
    "    processor = processors[task]()\n",
    "    output_mode = output_modes[task]\n",
    "    \n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = os.path.join(args.data_dir, 'cached_{}_{}_{}_{}'.format(\n",
    "        'dev' if evaluate else 'train',\n",
    "        list(filter(None, args.model_name)).pop(),\n",
    "        str(args.max_seq_length),\n",
    "        str(task)))\n",
    "    if os.path.exists(cached_features_file):\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
    "        label_list = processor.get_labels()\n",
    "        examples = processor.get_dev_examples(args.data_dir) if evaluate else processor.get_train_examples(args.data_dir)\n",
    "        features = convert_examples_to_features(examples, label_list, args.max_seq_length, tokenizer, output_mode,\n",
    "            cls_token_at_end = False,            # xlnet has a cls token at the end\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            cls_token_segment_id = 1,\n",
    "            pad_on_left= 0,                 # pad on the left for xlnet\n",
    "            pad_token_segment_id = 0\n",
    "                )                                    \n",
    "        if args.local_rank == [-1, 0]:\n",
    "            logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "            torch.save(features, cached_features_file)\n",
    "\n",
    "                       \n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    \n",
    "#     print(features[0].input_ids)\n",
    "#     print(features[0].masked_lm_labels)\n",
    "#     print(features[0].first_token_id)\n",
    "    \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_masked_lm_labels = torch.tensor([f.masked_lm_labels for f in features], dtype=torch.long)\n",
    "    all_first_token_id = torch.tensor([f.first_token_id for f in features], dtype=torch.long)\n",
    "    all_second_token_id = torch.tensor([f.second_token_id for f in features], dtype=torch.long)\n",
    "    \n",
    "    if output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_masked_lm_labels, all_label_ids, all_first_token_id,all_second_token_id)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2019 04:39:27 - INFO - __main__ -   Creating features from dataset file at .\\examples\\glue_data\\WINO\n",
      "07/27/2019 04:39:27 - INFO - __main__ -   Writing example 0 of 279\n",
      "07/27/2019 04:39:28 - INFO - __main__ -   ***** Running training *****\n",
      "07/27/2019 04:39:28 - INFO - __main__ -     Num examples = 279\n",
      "07/27/2019 04:39:28 - INFO - __main__ -     Num Epochs = 1\n",
      "07/27/2019 04:39:28 - INFO - __main__ -     Instantaneous batch size per GPU = 4\n",
      "07/27/2019 04:39:28 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "07/27/2019 04:39:28 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
      "07/27/2019 04:39:28 - INFO - __main__ -     Total optimization steps = 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Epoch:   0%|                                                                                  | 0/1 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration:   0%|                                                                             | 0/70 [00:00<?, ?it/s]07/27/2019 04:39:28 - INFO - __main__ -    global_step = 2, average loss = 4.893847703933716\n",
      "07/27/2019 04:39:28 - INFO - __main__ -   Saving model checkpoint to .\\examples\\glue_data\\WNLI_out\n",
      "07/27/2019 04:39:28 - INFO - pytorch_transformers.modeling_utils -   loading configuration file .\\examples\\glue_data\\WNLI_out\\config.json\n",
      "07/27/2019 04:39:28 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/27/2019 04:39:28 - INFO - pytorch_transformers.modeling_utils -   loading weights file .\\examples\\glue_data\\WNLI_out\\pytorch_model.bin\n",
      "07/27/2019 04:39:32 - INFO - pytorch_transformers.tokenization_utils -   Model name '.\\examples\\glue_data\\WNLI_out' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc). Assuming '.\\examples\\glue_data\\WNLI_out' is a path or url to a directory containing tokenizer files.\n",
      "07/27/2019 04:39:32 - INFO - pytorch_transformers.tokenization_utils -   loading file .\\examples\\glue_data\\WNLI_out\\added_tokens.json\n",
      "07/27/2019 04:39:32 - INFO - pytorch_transformers.tokenization_utils -   loading file .\\examples\\glue_data\\WNLI_out\\special_tokens_map.json\n",
      "07/27/2019 04:39:32 - INFO - pytorch_transformers.tokenization_utils -   loading file .\\examples\\glue_data\\WNLI_out\\vocab.txt\n",
      "07/27/2019 04:39:32 - INFO - __main__ -   Evaluate the following checkpoints: ['.\\\\examples\\\\glue_data\\\\WNLI_out']\n",
      "07/27/2019 04:39:32 - INFO - pytorch_transformers.modeling_utils -   loading configuration file .\\examples\\glue_data\\WNLI_out\\config.json\n",
      "07/27/2019 04:39:32 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "07/27/2019 04:39:32 - INFO - pytorch_transformers.modeling_utils -   loading weights file .\\examples\\glue_data\\WNLI_out\\pytorch_model.bin\n",
      "07/27/2019 04:39:35 - INFO - __main__ -   Creating features from dataset file at .\\examples\\glue_data\\WINO\n",
      "07/27/2019 04:39:35 - INFO - __main__ -   Writing example 0 of 279\n",
      "07/27/2019 04:39:35 - INFO - __main__ -   ***** Running evaluation  *****\n",
      "07/27/2019 04:39:35 - INFO - __main__ -     Num examples = 279\n",
      "07/27/2019 04:39:35 - INFO - __main__ -     Batch size = 4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:   0%|                                                                            | 0/70 [00:00<?, ?it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:   3%|█▉                                                                  | 2/70 [00:00<00:04, 15.67it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:   6%|███▉                                                                | 4/70 [00:00<00:04, 15.59it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:   9%|█████▊                                                              | 6/70 [00:00<00:04, 15.62it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  11%|███████▊                                                            | 8/70 [00:00<00:03, 15.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  14%|█████████▌                                                         | 10/70 [00:00<00:03, 15.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  17%|███████████▍                                                       | 12/70 [00:00<00:03, 15.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  20%|█████████████▍                                                     | 14/70 [00:00<00:03, 15.58it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  23%|███████████████▎                                                   | 16/70 [00:01<00:03, 15.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  26%|█████████████████▏                                                 | 18/70 [00:01<00:03, 14.82it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  29%|███████████████████▏                                               | 20/70 [00:01<00:03, 15.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  31%|█████████████████████                                              | 22/70 [00:01<00:03, 15.18it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  34%|██████████████████████▉                                            | 24/70 [00:01<00:03, 15.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  37%|████████████████████████▉                                          | 26/70 [00:01<00:02, 15.04it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  40%|██████████████████████████▊                                        | 28/70 [00:01<00:02, 14.91it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  43%|████████████████████████████▋                                      | 30/70 [00:01<00:02, 15.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  46%|██████████████████████████████▋                                    | 32/70 [00:02<00:02, 15.00it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  49%|████████████████████████████████▌                                  | 34/70 [00:02<00:02, 14.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  51%|██████████████████████████████████▍                                | 36/70 [00:02<00:02, 14.97it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  54%|████████████████████████████████████▎                              | 38/70 [00:02<00:02, 15.18it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  57%|██████████████████████████████████████▎                            | 40/70 [00:02<00:01, 15.25it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  60%|████████████████████████████████████████▏                          | 42/70 [00:02<00:01, 15.37it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  63%|██████████████████████████████████████████                         | 44/70 [00:02<00:01, 15.46it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  66%|████████████████████████████████████████████                       | 46/70 [00:03<00:01, 15.45it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  69%|█████████████████████████████████████████████▉                     | 48/70 [00:03<00:01, 15.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  71%|███████████████████████████████████████████████▊                   | 50/70 [00:03<00:01, 15.43it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  74%|█████████████████████████████████████████████████▊                 | 52/70 [00:03<00:01, 15.43it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  77%|███████████████████████████████████████████████████▋               | 54/70 [00:03<00:01, 15.43it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  80%|█████████████████████████████████████████████████████▌             | 56/70 [00:03<00:00, 15.32it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  83%|███████████████████████████████████████████████████████▌           | 58/70 [00:03<00:00, 15.42it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  86%|█████████████████████████████████████████████████████████▍         | 60/70 [00:03<00:00, 15.46it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  89%|███████████████████████████████████████████████████████████▎       | 62/70 [00:04<00:00, 15.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  91%|█████████████████████████████████████████████████████████████▎     | 64/70 [00:04<00:00, 15.50it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  94%|███████████████████████████████████████████████████████████████▏   | 66/70 [00:04<00:00, 15.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating:  97%|█████████████████████████████████████████████████████████████████  | 68/70 [00:04<00:00, 15.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Evaluating: 100%|███████████████████████████████████████████████████████████████████| 70/70 [00:04<00:00, 15.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False  True  True False False False False  True  True  True  True\n",
      " False  True  True  True  True  True False False False False False  True\n",
      "  True  True False False  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True False False False False  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True False False False False\n",
      "  True  True False False False False  True  True False False False False\n",
      "  True  True  True  True False False False False False False False False\n",
      " False False  True  True  True  True False False  True  True  True  True\n",
      "  True  True  True  True  True  True False False False False False False\n",
      "  True  True False False  True  True  True  True  True False  True  True\n",
      "  True  True False False  True  True False False False False False False\n",
      " False False  True  True False False  True  True  True  True  True  True\n",
      " False False  True  True False False  True  True False False  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      " False False False False  True False False False False False False False\n",
      " False False False False  True  True  True  True  True  True False False\n",
      " False False False False False False False False False False False False\n",
      " False False  True  True False False False False False False False False\n",
      "  True  True False False  True  True False False False False  True  True\n",
      "  True  True  True  True False False False False  True  True False False\n",
      " False False  True  True False False  True  True False False False False\n",
      " False False False False False  True  True False False False False False\n",
      " False  True  True False False False False False False  True  True  True\n",
      "  True  True  True]\n",
      "[0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0\n",
      " 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 1\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "07/27/2019 04:39:40 - INFO - __main__ -   ***** Eval results  *****\n",
      "07/27/2019 04:39:40 - INFO - __main__ -     acc = 0.5053763440860215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acc_': 0.5053763440860215}\n"
     ]
    }
   ],
   "source": [
    "# Do training\n",
    "\n",
    "# Training\n",
    "if args.do_train:\n",
    "    train_dataset = load_and_cache_examples(args, args.task_name, tokenizer, evaluate=False)\n",
    "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
    "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n",
    "\n",
    "\n",
    "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "    # Create output directory if needed\n",
    "    if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n",
    "    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "    # They can then be reloaded using `from_pretrained()`\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(args.output_dir)\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "\n",
    "    # Good practice: save your training arguments together with the trained model\n",
    "    torch.save(args, os.path.join(args.output_dir, 'training_args.bin'))\n",
    "\n",
    "    # Load a trained model and vocabulary that you have fine-tuned\n",
    "\n",
    "    model = BertForMaskedLM.from_pretrained(args.output_dir)\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.output_dir)\n",
    "    model.to(args.device)\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "results = {}\n",
    "if args.do_eval and args.local_rank in [-1, 0]:\n",
    "    checkpoints = [args.output_dir]\n",
    "    if args.eval_all_checkpoints:\n",
    "        checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "        logging.getLogger(\"pytorch_transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "    logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "    for checkpoint in checkpoints:\n",
    "        global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
    "        model = BertForMaskedLM.from_pretrained(args.output_dir)\n",
    "        model.to(args.device)\n",
    "        result = evaluate(args, model, tokenizer, prefix=global_step)\n",
    "        result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n",
    "        results.update(result)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
